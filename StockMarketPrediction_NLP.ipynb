{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b02136d",
   "metadata": {
    "id": "2b02136d"
   },
   "source": [
    "#     **Text Mining**\n",
    "##     **NOVA IMS**\n",
    "####     **Group 013:** Carlota Reis - 20211208 | Guilherme Miranda - 20210420  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad8946",
   "metadata": {
    "id": "57ad8946"
   },
   "source": [
    "## **Stock Market Prediction**\n",
    "## Predicting stock market movement from news text\n",
    "### This notebook uses the dateset *test.csv*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ffbb0c",
   "metadata": {
    "id": "37ffbb0c"
   },
   "source": [
    "## **Dataset description**\n",
    "\n",
    "- **ID**: unique identifier of each line/day\n",
    "- **Headline**: news headlines ranging from \"Headline1\" to \"Headline25\". For each line, you should use these columns’ text (you are not required to use all columns) to predict the “Closing Status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc52cbe3",
   "metadata": {
    "id": "dc52cbe3"
   },
   "source": [
    "## **Packages and Downloads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7691f83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7691f83",
    "outputId": "3c7244e0-884d-4d8e-a415-6a5af99b64eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lotar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lotar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lotar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lotar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stopwords.words('english')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a2ef20",
   "metadata": {
    "id": "87a2ef20"
   },
   "source": [
    "## **Data Import and Transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61eead99",
   "metadata": {
    "id": "61eead99"
   },
   "outputs": [],
   "source": [
    "#Importing datasets\n",
    "stocks_df_test = pd.read_csv('test.csv')\n",
    "stocks_df_train = pd.read_csv('train.csv')\n",
    "\n",
    "#Transforming the datasets into dataframes\n",
    "stocks_df_test = pd.DataFrame(data=stocks_df_test)\n",
    "stocks_df_train = pd.DataFrame(data=stocks_df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db1dd3",
   "metadata": {},
   "source": [
    "### *Train*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c70d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating New Dataframe\n",
    "train_df = stocks_df_train\n",
    "\n",
    "#Appending all the Headlines into 1 column\n",
    "train_df_aux = []\n",
    "for row in range(0,1690):\n",
    "    train_df_aux.append(' '.join(str(x) for x in train_df.iloc[row,2:26]))\n",
    "\n",
    "#Adding this column to dataframe\n",
    "train_df['Sum'] = train_df_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9abeb8",
   "metadata": {},
   "source": [
    "### *Test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d101695",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8d101695",
    "outputId": "841d05ef-12fc-49f3-a999-bfe9fa79b3d8"
   },
   "outputs": [],
   "source": [
    "#Creating New Dataframe\n",
    "test_df = stocks_df_test\n",
    "\n",
    "#Appending all the Headlines into 1 column\n",
    "test_df_aux = []\n",
    "for row in range(0,299):\n",
    "    test_df_aux.append(' '.join(str(x) for x in test_df.iloc[row,1:25]))\n",
    "\n",
    "#Adding this column to dataframe\n",
    "test_df['Sum'] = test_df_aux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532d00bf",
   "metadata": {
    "id": "532d00bf"
   },
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9733c545",
   "metadata": {
    "id": "9733c545"
   },
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69e2feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_clean(text_list):\n",
    "    \n",
    "    updates = []\n",
    "    \n",
    "    for j in tqdm(text_list):\n",
    "        \n",
    "        text = j\n",
    "        \n",
    "        #LOWERCASE TEXT\n",
    "        text = text.lower()\n",
    "        \n",
    "        #REMOVE IDENTIFIED ISSUES IN THE CORPUS\n",
    "        text = re.sub(\"b\\\"\", \"\", text)\n",
    "        text = re.sub(\"b'\", \"\", text)\n",
    "        text = re.sub(\"u.s.\",\"us\", text)\n",
    "        text = re.sub(\"u.s\",\"us\", text)\n",
    "        text = re.sub(\"us.\",\"us\", text)\n",
    "        text = re.sub(\"u.k.\",\"uk\", text)\n",
    "        text = re.sub(\"u.k\",\"uk\", text)\n",
    "        text = re.sub(\"uk.\",\"uk\", text)\n",
    "        text = re.sub(\"u.n.\",\"un\", text)\n",
    "        text = re.sub(\"u.n\",\"un\", text)\n",
    "        text = re.sub(\"un.\",\"un\", text)\n",
    "        \n",
    "\n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed0040b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text_list):\n",
    "    \n",
    "    updates = []\n",
    "    \n",
    "    for j in tqdm(text_list):\n",
    "        \n",
    "        text = j\n",
    "        \n",
    "      \n",
    "        #REMOVE NUMERICAL DATA and PUNCTUATION\n",
    "        text = re.sub(\"[^a-zA-Z]\",\" \", text )\n",
    "\n",
    "        #REMOVE STOPWORDS\n",
    "        text = \" \".join([word for word in text.split() if word not in stop])\n",
    "                \n",
    "        #Lemmatize\n",
    "        text = \" \".join(lemma.lemmatize(word) for word in text.split())\n",
    "        \n",
    "        #Stemming\n",
    "        text = \" \".join(stemmer.stem(word) for word in text.split())\n",
    "            \n",
    "        updates.append(text)\n",
    "        \n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b0e4fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 299/299 [00:00<00:00, 27176.13it/s]\n",
      "100%|██████████| 1690/1690 [00:00<00:00, 31289.86it/s]\n",
      "100%|██████████| 299/299 [00:00<00:00, 347.22it/s]\n",
      "100%|██████████| 1690/1690 [00:04<00:00, 346.00it/s]\n"
     ]
    }
   ],
   "source": [
    "#Pre-Clean\n",
    "test_df_clean = pre_clean(test_df['Sum'])\n",
    "train_df_clean = pre_clean(train_df['Sum'])\n",
    "\n",
    "#Clean\n",
    "test_df_clean = clean(test_df_clean)\n",
    "train_df_clean = clean(train_df_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c03dcd",
   "metadata": {
    "id": "f1c03dcd"
   },
   "source": [
    "# **Feature Engineering - Bag of Words (BoW)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "iJSAbGX5eFbi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJSAbGX5eFbi",
    "outputId": "7efe0e29-e608-45bc-ddf3-f1cf53d74455"
   },
   "outputs": [],
   "source": [
    "bow = CountVectorizer(binary=True)\n",
    "\n",
    "#Defining independent variables\n",
    "X = bow.fit_transform(train_df_clean)\n",
    "\n",
    "#Defining dependent variable\n",
    "y = np.array(train_df['Closing Status'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5f4fd6",
   "metadata": {},
   "source": [
    "# **Classification Model - KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a82acca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(metric='cosine', n_neighbors=10, weights='distance')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train Classifier\n",
    "modelknn = KNeighborsClassifier(n_neighbors = 10, metric = 'cosine', weights = 'distance')\n",
    "\n",
    "#Fitting the model\n",
    "modelknn.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "92535cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the independent\n",
    "X_test = bow.transform(test_df_clean)\n",
    "\n",
    "\n",
    "#Predicting for test set\n",
    "y_pred = modelknn.predict(X_test)\n",
    "\n",
    "#If we want the values of the predictions, perform this\n",
    "\n",
    "test_df['Pred_KNN_BoW'] = y_pred.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48122579",
   "metadata": {},
   "source": [
    "# **Final predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79607117",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test_df[['Id','Pred_KNN_BoW']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1432e3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv file\n",
    "predictions.to_csv('Predictions_013.csv', index=False, sep = ';')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "95497283",
    "d87c0443"
   ],
   "name": "Experiments_013_v3_all_headlines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
