# StockMarketPrediction_NLP

Text Mining 
Stock Market Prediction 
Predicting Stock Market Movement from News Text 
 
 
Project consisted on developing an NLP model to predict the daily closing values of a stock market index based on news. Resorting to the NLP techniques learned during the course, it was required to implement a binary classifier that, for each day, receives news headlines and is able to predict if the index closing value rose (1) or decreased (0).

The jupyter notenook presents all the experiments and techniques used, as well as the testing of each implemented model and the performance achived with each one. 
 
#1.	Data Exploration 
 
The purpose of the project consists in developing an NLP model, based on the classes’ techniques, capable of predicting the daily closing status of a stock market index, according to some news headlines provided in the training set (train.csv), and then test it in the test set (test.csv). 
The training dataset contains the “ID”, which is the unique identifier of each line/day, the “Closing Status”, which is the dependent variable, that takes the value of 1, if the index closing price increased or stayed the same, and 0, which indicates that the index price decreased. Lastly, it has 25 columns, with daily news headlines, from “Headline1” to “Headline25”. 
Firstly, some essential libraries were to develop the model in Python such as pandas, numpy, matplotlib, NLTK, Scikit Learn and others and also the provided datasets. 
Then, it was initiated the Data Exploration phase, which is crucial in understanding the dataset and achieving some early conclusions, by checking some keys statistics and graphical visualization. 
The train dataset contains data from 1,690 days (approximately 6,76 years)  and the test set has data of 299 days (approximately 1,20 years). 
In terms of missing values, it was detected 3 missing values in Headline 3 and Headline 11 and 1 missing value in Headline 23. Since all headlines were grouped into a single column, and later cleaned the merged text, we considered all news headlines’ columns. 
Considering the Closing Status, it is possible to observe that in 53,37% of the days the index price increased or stayed the same against 46,67% of the days, which was verified a decrease in the index price. Hence one can conclude that the data (training set) is not really unbalanced. 
Subsequently, it was obtained some statistics about the words in the corpus. The average number of words presented in the merged news headlines was approximately 426 words with a standard deviation of 70. The median obtained was 427 words and the lengthiest merged headlines had 660 words. Considering the most frequent words before applying some data processing, it was possible to observe that the majority are stop words, which at this point, does not provide any valuable information regarding the news headlines and the index daily evolution.  
Before diving into the preprocessing phase, the data was split into 2 sets: training and test, which was split with the criteria of ¾ of the data being the training set (1267 days) and ¼ (423 days) being the test set.  

#2.	Data Preprocessing 
 
In order to develop a model capable of predicting the index daily direction according to some news headlines, it is important to first treat the data. The NLP is a key field essential in translating the “human language” into computer inputs, in pursuance of material conclusions about text information. 
Some preprocessing methods were applied to the data, which predominantly consists of the function provided in the class. This function applies several preprocessing methods at once, requiring only strings as inputs. The mentioned methods consist in lowercasing the text, which eliminates the differentiation of the same word written with an upper or lower case. Also, it was removed numerical values and punctuation, by only considering letters from A to Z as characters (it seems that the data is written in English). After these two processes, as extra work, considering what we’ve seen in the Data Exploration phase, the most frequent words are stop words, which don’t provide essential information about the text and hence they were eliminated. Lastly, as extra work, the stemming and lemmatization procedures were applied, by reducing the words into their simplest form and meaning. After applying these methods, we obtained some odd words as the most frequent ones, which were the letter b and letter u. By checking the initial dataframe, it seems that the “b’s” were related to some sort formatting of the news headlines, probably standing for “bold”. So, as extra work, it was performed some “pre-cleaning” where it was replaced the letter “b” after single and double quotes. Subsequently, the letter “u” was the most frequent one. After analyzing the initial dataframe, it suggested that it was related to the abbreviations of United States, United Kingdom and United Nations. In those situations, the abbreviation is made with a dot between the letter “u” and the next capital letter, so these situations were also considered in the “pre cleaning” phase.  
After applying these pre-cleaning replacements, those situations disappeared, which resulted in having more valuable words as the most frequent ones.  
To implement some classification models, it was necessary to implement some feature engineering to the train and test data. The techniques considered were the Bag of Words and the Term Frequency – Inverse Document Frequency. As extra work, it was also applied the Word Embeddings method and lastly, the Pad Sequences, which is relies on the previous. 
Briefly exploring what each of these feature engineering methods consist of, the Bag of Words is a vector space model that considers each word of the corpus as one feature. It classifies the presence or the absence of a word in a document as 1 or 0, respectively. Hence, it performs a binary classification of the word’s frequency of each document and transforms these values into vectors. Considering the distance between the different vectors, it possible to conclude that the closest ones share more information between them. The major limitations of the BoW are, essentially, the dimensionality (each word is a feature), the fact that it doesn’t consider the order of the words in a sentence and also that it can’t relate some words, such as identifying countries, unit measures, etc.  
The TF-IDF technique is the result of two concepts which may be somewhat contradictory. First it considers the raw frequency of each word in a document (term-frequency) and secondly it gives importance to words that appear few in document. Basically, it gives a higher weight to words that occur few in documents and identifies which documents add more information than the ones who share the same information. To both components is applied the log10, since the data usually comprises tons of documents. 
The Word Embeddings method constructs short and dense vectors that have similar semantic properties. In this case, due to computational limitations, it was downloaded the “glove-twitter-25 “dictionary – as the one considered to be most suitable for this problem – “word2vec-google-news-300” was too large. Through this feature engineering, the model can identify semantic relations between words and associations through the coisine similarity. As the name suggests, the words are also represented through vectors.  
 

#3.	Classification Models 
 
Concerning the classification models, the three considered were the K-Nearest Neighbors (with Bag of Words and TF-IDF), the Naïve Bayes and the Logistic Regression (with TF-IDF and Word Embeddings). As extra work, the Multilayer Perceptron (with also TF-IDF and Word Embeddings) and  the Long Short-Term Memory were implemented.  
The K-Nearest Neighbor model was applied using the BoW features and later through the TF-IDF. This model finds the K (it was applied a K of 10) nearest matches between the data points, using a distance metric (in this case, the coisine similarity).  
The second model employed was the Naïve Bayes, which is a probabilistic classifier, based on conditional probabilities.  
The logistic regression (computed using both the TF-IDF and word embeddings features) consists of fitting a logistic regression through the various datapoints, meaning, finding a line that minimizes the residuals (difference between the real-points and the predicted ones). 
As for the Multilayer Perceptron model, it is a class of feedforward networks and consists of nodes with several layers (at least three): an input one, a hidden one and the output one. Through an activation function (commonly being sigmoids), the inputs given are processed through, retrieving the output. This is done by using backpropagation to train the network. 
Finally, the Long-Short-Term Memory model, unlike the MLP, follows a recurrent neural network architecture. It is composed by a hidden state and new inputs, a forget gate (controls what information should be discarded), an input gate (crucial to track which elements are important), an update cell state (influenced by the previous results) and an update hidden cell.  Basically, the models is able to learn the order dependence in sequence prediction problems. 
  
 

#4.	Evaluation and Results 
 
All models’ performance were very similar, retrieving, on average, values between 0,30 and 0,54 for the precision, values ranging between 0,47 and 0,55 concerning recall and between 0,39 and 0,52 for the F1-Score (which were expected values as the F1-Score is the harmonic average between precision and recall). 
From all the models evaluated, using the different feature engineering techniques, 4 of them presented relatively superior performance: the KNN using  the Bag of Words, the KNN using the TF-IDF method, the Logistic Regression computed also using the TF-IDF feature and the MLP using the TF-IDF. These results can be observed below, on Table 1. 
  
By briefly analysing the results, the KNN-BoW presents the highest precision, meaning that it could predict 54% of True Positive observations, considering the universe of true positives and false positives. As for the recall, the best performing model was the MLP-TF-IDF, with a value of 55%. This means that the model was able to correctly predict 55% of the actual positives, in the universe of the true positives and false negatives. Considering the problem the model is trying to solve, a true positive value means that the model predicted that, for a certain day, the index price would maintain or increase and that was the verified outcome. Finally, being the F1-Score the harmonic average between the recall and the precision, the model KNN-BoW model was the one retrieving the highest value for this performance measure. 
Given these results, the chosen NLP model to predict the daily closing direction of a stock market index based on news headlines was the KNN-BoW. 
 
